{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc100147",
   "metadata": {},
   "source": [
    "### Assignment 1\n",
    "\n",
    "The folder `results` contains the result of 11 density functional theory (DFT) calculations of mine. The goal of this exercise is to traverse the files in these folders and create a summary dictionary and save it as a JSON file. To do so:\n",
    "\n",
    "- Traverse the subfolders in the `results` folder with the function `walk` of the built-in `os` Python package (see info [here](https://docs.python.org/3/library/os.html#os.walk)) and store all file paths in a list. For working with paths you may want to consider using the `pathlib` package (see info [here](https://www.pythonmorsels.com/pathlib-module/)), but this is optional (plain strings with the `os.path.join` function work too).\n",
    "- Loop through the path list and open the `log.txt` and `duration.txt` files with a context manager (i.e., use: `with open(\"log.txt\", \"r\") as f:`). Further information on opening and writing files can be found [here](https://www.geeksforgeeks.org/python/difference-between-modes-a-a-w-w-and-r-in-built-in-open-function/) and [here](https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files). \n",
    "    - Inside a `log.txt` file: Search through the lines of text for the *\"the Fermi energy is\"* and the *\"total energy\"* and save each value in a variable. (see screenshot below) Remember the string methods `split` and `strip`, which may be helpful to clean up the string and then convert it to a float value.\n",
    "\n",
    "    ![Relevant information of log.txt file](./dft_log_example.jpg)\n",
    "    - Inside a `duration.txt` file: Take the first line which is of form `hour:minutes:seconds` and convert it to seconds and save it in an integer variable.\n",
    "- Be careful about the case that the required information is not available (one of the files encountered an error during DFT calculation, and hence the required values are not available). One option is to use the `try` and `except` approach. Another approach is to use the `finally` clause in the `for` loop that contains a `break` statement once a value is found (and if no value is found, the `finally` block will run).\n",
    "- Save all the collected information from all folders in a dictionary that has the folder name as an integer key and the corresponding value is another dictionary that contains the keys \"duration_seconds\" (with an integer value), \"Fermi_energy\" (with a float value), \"total_energy\" (with a float value), and \"error\" (with a boolean value). The final dictionary should look something like this (the values may of course look different, but the structure should look basically the same):\n",
    "```Python\n",
    "results = {\n",
    "    0: {\n",
    "        \"duration_seconds\": 478, \n",
    "        \"Fermi_energy\": 4.743, \n",
    "        \"total_energy\": -7056.654239, \n",
    "        \"error\": False\n",
    "        }, \n",
    "    1: {\n",
    "        \"duration_seconds\": 1289, \n",
    "        \"Fermi_energy\": 7.047, \n",
    "        \"total_energy\": -6429.00935, \n",
    "        \"error\": False\n",
    "        },\n",
    "    2: {\n",
    "        \"duration_seconds\": None, \n",
    "        \"Fermi_energy\": None, \n",
    "        \"total_energy\": None, \n",
    "        \"error\": True\n",
    "        },\n",
    "     ...\n",
    "     }\n",
    "```\n",
    "- Finally, save that dictionary to a JSON file using the built-in `json` package utilizing a context manager (similar to how you opened files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f561e8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 files under 'results'\n",
      "Path list:\n",
      "results/9/log.txt\n",
      "results/9/duration.txt\n",
      "results/0/log.txt\n",
      "results/0/duration.txt\n",
      "results/7/log.txt\n",
      "results/7/duration.txt\n",
      "results/6/log.txt\n",
      "results/6/duration.txt\n",
      "results/1/log.txt\n",
      "results/1/duration.txt\n",
      "results/10/log.txt\n",
      "results/10/duration.txt\n",
      "results/8/log.txt\n",
      "results/8/duration.txt\n",
      "results/4/log.txt\n",
      "results/4/duration.txt\n",
      "results/3/log.txt\n",
      "results/3/duration.txt\n",
      "results/2/log.txt\n",
      "results/2/duration.txt\n",
      "results/5/log.txt\n",
      "results/5/duration.txt\n"
     ]
    }
   ],
   "source": [
    "# Traverse the 'results' folder and collect all file paths\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "results_dir = Path(\"results\")\n",
    "file_paths = []\n",
    "for root, dirs, files in os.walk(results_dir):\n",
    "    for fname in files:\n",
    "        file_paths.append(str(Path(root) / fname))\n",
    "\n",
    "# Print the collected file paths\n",
    "print(f\"Found {len(file_paths)} files under '{results_dir}'\")\n",
    "print(\"Path list:\")\n",
    "for p in file_paths[:]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd719005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 11 folders. Summary saved to results_summary.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "results_summary = {}\n",
    "results_dir = Path(\"results\")\n",
    "\n",
    "for sub in sorted(results_dir.iterdir(), key=lambda p: int(p.name) if p.name.isdigit() else p.name):\n",
    "    if not sub.is_dir():\n",
    "        continue\n",
    "    key = int(sub.name) if sub.name.isdigit() else sub.name\n",
    "    fermi = None\n",
    "    total = None\n",
    "    duration_seconds = None\n",
    "    error = False\n",
    "\n",
    "    # Parse log.txt\n",
    "    log_path = sub / \"log.txt\"\n",
    "    try:\n",
    "        with open(log_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                if 'the Fermi energy is' in line:\n",
    "                    parts = line.split()\n",
    "                    try:\n",
    "                        # assume last token is the numeric value\n",
    "                        fermi = float(parts[-1])\n",
    "                    except Exception:\n",
    "                        try:\n",
    "                            # try second last\n",
    "                            fermi = float(parts[-2])\n",
    "                        except Exception:\n",
    "                            fermi = None\n",
    "                if 'total energy' in line:\n",
    "                    parts = line.split()\n",
    "                    for tok in reversed(parts):\n",
    "                        try:\n",
    "                            total = float(tok)\n",
    "                            break\n",
    "                        except Exception:\n",
    "                            continue\n",
    "    except FileNotFoundError:\n",
    "        error = True\n",
    "    except Exception:\n",
    "        error = True\n",
    "\n",
    "    # Parse duration.txt\n",
    "    duration_path = sub / \"duration.txt\"\n",
    "    try:\n",
    "        with open(duration_path, \"r\") as f:\n",
    "            first = f.readline().strip()\n",
    "            if first:\n",
    "                # expected format H:M:S\n",
    "                try:\n",
    "                    h, m, s = [int(x) for x in first.split(\":\")]\n",
    "                    duration_seconds = h*3600 + m*60 + s\n",
    "                except Exception:\n",
    "                    duration_seconds = None\n",
    "    except FileNotFoundError:\n",
    "        error = True\n",
    "    except Exception:\n",
    "        error = True\n",
    "\n",
    "    # If any required value is missing, mark error True\n",
    "    if fermi is None or total is None or duration_seconds is None:\n",
    "        error = True\n",
    "\n",
    "    results_summary[key] = {\n",
    "        \"duration_seconds\": duration_seconds,\n",
    "        \"Fermi_energy\": fermi,\n",
    "        \"total_energy\": total,\n",
    "        \"error\": error\n",
    "    }\n",
    "\n",
    "# Save to JSON\n",
    "out_path = Path(\"results_summary.json\")\n",
    "with open(out_path, \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"Parsed {len(results_summary)} folders. Summary saved to {out_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df83ace",
   "metadata": {},
   "source": [
    "### Assignment 2\n",
    "\n",
    "Utilize Python to automate a task you have done in the past. This can be anything you previously did manually, but now can automate using Python. A few random examples, to give you an idea of what that could be:\n",
    "\n",
    "- Read some CSV output files that you obtained by an experimental device and do basic analysis. Can use the `csv` package here. For XLSX files there is a library called `openpyxl`, but it is not a built-in library (i.e., you would need to install it). It may be easier to convert your XLSX files to CSV, in that case.\n",
    "- Copy files from one folder to another (e.g., for backup or sorting purposes). This could utilize the `os`, `pathlib`, and `shutil` built-in packages.\n",
    "- Using the `input` statement you could write a basic Python script that converts one set of units to another set of units.\n",
    "- Be creative! Anything you have done previously that required manual repetition on a computer/files, can be automated with Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af902de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV analyzed: SI HAADF 20250723 1334 88000 x.csv -> summary saved to csv_summary_SI_HAADF_20250723.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from statistics import mean, median, pstdev\n",
    "\n",
    "csv_path = Path(\"SI HAADF 20250723 1334 88000 x.csv\")\n",
    "summary = {}\n",
    "\n",
    "with open(csv_path, newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    rows = list(reader)\n",
    "\n",
    "# Create column names\n",
    "# Use second row as names, third row as units\n",
    "col_names = [c.strip('\"') for c in rows[0]]\n",
    "units = [c.strip('\"') for c in rows[2]]\n",
    "\n",
    "# data starts at row index 3\n",
    "data_rows = rows[3:]\n",
    "# Convert to floats where possible\n",
    "cols = list(zip(*data_rows))\n",
    "num_cols = {}\n",
    "for i, col in enumerate(cols):\n",
    "    nums = []\n",
    "    for v in col:\n",
    "        try:\n",
    "            nums.append(float(v))\n",
    "        except Exception:\n",
    "            pass\n",
    "    num_cols[i] = nums\n",
    "\n",
    "# Build summary\n",
    "summary['file'] = str(csv_path)\n",
    "summary['n_rows'] = len(data_rows)\n",
    "summary['columns'] = []\n",
    "for i, name in enumerate(col_names):\n",
    "    col_summary = {\n",
    "        'index': i,\n",
    "        'name': name.strip('\"'),\n",
    "        'unit': units[i].strip('\"') if i < len(units) else None,\n",
    "        'numeric': len(num_cols[i]) > 0\n",
    "    }\n",
    "    if col_summary['numeric']:\n",
    "        vals = num_cols[i]\n",
    "        col_summary.update({\n",
    "            'count': len(vals),\n",
    "            'mean': mean(vals),\n",
    "            'median': median(vals),\n",
    "            'std_pop': pstdev(vals) if len(vals) > 1 else 0.0,\n",
    "            'min': min(vals),\n",
    "            'max': max(vals)\n",
    "        })\n",
    "    summary['columns'].append(col_summary)\n",
    "\n",
    "# capture head (first 10 data rows)\n",
    "summary['head'] = [row for row in data_rows[:10]]\n",
    "\n",
    "out_path = Path(\"csv_summary_SI_HAADF_20250723.json\")\n",
    "with open(out_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"CSV analyzed: {csv_path} -> summary saved to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6459ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/Users/emma/Documents/GitHub/ML4MSD-HW--PeiyunF-/Homework1/Homework/Homework1/SI HAADF 20250723 1334 88000 x.csv', '/Users/emma/Documents/GitHub/ML4MSD-HW--PeiyunF-/Homework1/Homework/Homework1/BackUp/SI HAADF 20250723 1334 88000 x.csv', 'copy')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('/Users/emma/Documents/GitHub/ML4MSD-HW--PeiyunF-/Homework1/Homework/Homework1/SI HAADF 20250723 1334 88000 x.csv',\n",
       "  '/Users/emma/Documents/GitHub/ML4MSD-HW--PeiyunF-/Homework1/Homework/Homework1/BackUp/SI HAADF 20250723 1334 88000 x.csv',\n",
       "  'copy')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "import fnmatch\n",
    "\n",
    "\n",
    "def copy_files(src, dst, pattern='*', overwrite=False, dry_run=True):\n",
    "    \"\"\"Copy files matching pattern from src to dst.\n",
    "\n",
    "    Args:\n",
    "        src (str/Path): source directory\n",
    "        dst (str/Path): destination directory\n",
    "        pattern (str): glob or fnmatch pattern (e.g. '*.csv')\n",
    "        overwrite (bool): overwrite existing files\n",
    "        dry_run (bool): if True, only print actions\n",
    "    Returns:\n",
    "        list: list of (src_path, dst_path, action) tuples\n",
    "    \"\"\"\n",
    "    src = Path(src)\n",
    "    dst = Path(dst)\n",
    "    dst.mkdir(parents=True, exist_ok=True)\n",
    "    actions = []\n",
    "    for p in src.rglob(pattern):\n",
    "        if p.is_file():\n",
    "            target = dst / p.name\n",
    "            if target.exists() and not overwrite:\n",
    "                actions.append((str(p), str(target), 'skip'))\n",
    "                if not dry_run:\n",
    "                    continue\n",
    "            actions.append((str(p), str(target), 'copy'))\n",
    "            if not dry_run:\n",
    "                shutil.copy2(p, target)\n",
    "    for a in actions:\n",
    "        print(a)\n",
    "    return actions\n",
    "\n",
    "\n",
    "# Run actual copy (disable dry_run to perform copy)\n",
    "copy_files(\n",
    "    \"/Users/emma/Documents/GitHub/ML4MSD-HW--PeiyunF-/Homework1/Homework/Homework1\",\n",
    "    \"/Users/emma/Documents/GitHub/ML4MSD-HW--PeiyunF-/Homework1/Homework/Homework1/BackUp\",\n",
    "    pattern=\"*.csv\",\n",
    "    overwrite=False,\n",
    "    dry_run=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c57cead",
   "metadata": {},
   "source": [
    "### Assignment 3\n",
    "\n",
    "Go through the `ML4MSD-Files/Resources/Python_resources.md` file and summarize any interesting observations or things you learned from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec285dd",
   "metadata": {},
   "source": [
    "I did not go through all the resources yet and I will do! Thank you for the information Professor!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4msd-teacher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
